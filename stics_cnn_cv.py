# -*- coding: utf-8 -*-
"""SticsModel.iypnb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hcOZAifTV0oto7PwALcPTOlxPbI-fkMy
"""

import os
from PIL import Image
from numpy import asarray
import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import time
from sklearn.utils import shuffle
from sklearn import metrics
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import auc
from sklearn.metrics import precision_recall_curve

train_ratio = 0.7
valid_ratio = 0.3

num_epochs = 50
num_classes = 5
batch_size = 32
learning_rate = 0.0001

if torch.cuda.is_available():
  DEVICE = torch.device("cuda")
else:
  DEVICE = torch.device("cpu")


def setArgParser():
    parser = argparse.ArgumentParser(description="Select which organizational function you would like to use.")
    parser.add_argument("-g", "--generalized", help="Run generalized model (i.e. for combined dataset).", 
                        required=False, action='store_true')
    parser.add_argument("-p", "--personalized", help = "Run personalized model (i.e. for each individual separately).", 
                        required = False, action='store_true')
    return parser  

class SticsModel(nn.Module):
  def __init__(self):
    super(SticsModel, self).__init__()
    self.layer1 = nn.Sequential(
      nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=2),
      nn.ReLU(),
      nn.MaxPool2d(kernel_size=2, stride=2))
    self.layer2 = nn.Sequential(
      nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=2),
      nn.ReLU(),
      nn.MaxPool2d(kernel_size=2, stride=2))
    self.layer3 = nn.Sequential(
      nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2),
      nn.ReLU(),
      nn.MaxPool2d(kernel_size=2, stride=2))
    self.drop_out = nn.Dropout()
    self.flatten = nn.Flatten()
    self.fc1 = nn.Linear(115200, 1000)
    self.fc2 = nn.Linear(1000, 2)

  def forward(self, x):
    out = self.layer1(x)
    out = self.layer2(out)
    out = self.layer3(out)
    out = out.reshape(out.size(0), -1)
    out = self.drop_out(out)
    out = self.flatten(out)
    out = self.fc1(out)
    out = self.fc2(out)
    return out

root = '/cluster/home/t62928uhn/'
feature_table_csv = root + 'clinical_data_included/%s/shift_feature_table.csv'
train_ratio = 0.8

MODEL_STORE_PATH = root + 'model/usf_model3.pt'
usf_model = SticsModel()

def read_images_and_labels(profile):
  feature_table = pd.read_csv(feature_table_csv % profile)

  image_paths = feature_table['ID'].to_numpy()

  for i, image_path in enumerate(image_paths):
    split_path = image_path.split('/')
    image_path = 'rtls_data/' + profile + '/' + '/'.join(split_path[2:])
    # print("path: ", image_path)
    image_path = image_path.replace('.csv', '.png')
    image_paths[i] = image_path

  # image_paths = [image_path.replace('.csv', '.png') for image_path in image_paths]
  # labels = feature_table['Labels'].to_numpy()
  labels = np.array([1 if val > 2 else 0 for val in feature_table.Labels])
  images = [(Image.open(root + image_path)).resize((275, 389)) for image_path in image_paths]

  images2 = []
  for image in images:
    data = asarray(image)[:,:,:3]/255
    data = data.transpose(2, 0, 1)
    images2.append(data)
  
  X, y = shuffle(images2, labels, random_state=0)
  n_train = int(len(X) * train_ratio)
  
  label_vals = [0, 1]
  occurences = []
  cnt = 0

  x_train, y_train = X[:n_train], y[:n_train]
  x_test, y_test = X[n_train:], y[n_train:]

  for l in label_vals:
    occurences.append(y_train.tolist().count(l))

  weights = 1 / (occurences/np.sum(occurences))
  weights = torch.FloatTensor(weights).to(DEVICE)

  x_train_tensor = torch.Tensor(asarray(x_train))
  y_train_tensor = torch.LongTensor(asarray(y_train))

  x_test_tensor = torch.Tensor(asarray(x_test))
  y_test_tensor = torch.Tensor(asarray(y_test))

  x_train_tensor = x_train_tensor.to(DEVICE)
  y_train_tensor = y_train_tensor.to(DEVICE)
  x_test_tensor = x_test_tensor.to(DEVICE)
  y_test_tensor = y_test_tensor.to(DEVICE)
  return weights, x_train_tensor, x_test_tensor, y_train_tensor, y_test_tensor

def read_images_and_labels_cv(profile, combined=False):
  if combined:
    labels = []
    image_paths = np.array([])
    print("Combining data into a generalized dataset...")
    for p in profile:
      feature_table = pd.read_csv(feature_table_csv % p, index_col=False)
      labels = np.append(labels, feature_table.Labels)
      image_paths_ = feature_table['ID'].to_numpy()

      for i, image_path in enumerate(image_paths_):
        split_path = image_path.split('/')
        if '/download/' in image_path:
          print(split_path)
          image_path = 'rtls_data/' + p + '/' + '/'.join(split_path[6:])
        else:
          image_path = 'rtls_data/' + p + '/' + '/'.join(split_path[2:])
        # print("path: ", image_path)
        image_path = image_path.replace('.csv', '.png')
        image_paths_[i] = image_path
      image_paths = np.append(image_paths, image_paths_)
    print(image_paths)
  else:
    feature_table = pd.read_csv(feature_table_csv % profile)
    labels = feature_table.Labels.to_numpy()
    image_paths = feature_table['ID'].to_numpy()

    for i, image_path in enumerate(image_paths):
      split_path = image_path.split('/')
      image_path = 'rtls_data/' + profile + '/' + '/'.join(split_path[2:])
      # print("path: ", image_path)
      image_path = image_path.replace('.csv', '.png')
      image_paths[i] = image_path

  # image_paths = [image_path.replace('.csv', '.png') for image_path in image_paths]
  # labels = feature_table['Labels'].to_numpy()
  labels = np.array([1 if val > 2 else 0 for val in labels])
  images = [(Image.open(root + image_path)).resize((275, 389)) for image_path in image_paths]

  images2 = []
  for image in images:
    data = asarray(image)[:,:,:3]/255
    data = data.transpose(2, 0, 1)
    print(data.shape)
    images2.append(data)
  
  # X, y = shuffle(images2, labels, random_state=0)
  X, y = images2, labels
  X_tensor = torch.Tensor(asarray(X))
  y_tensor = torch.LongTensor(asarray(y))

  X_tensor = X_tensor.to(DEVICE)
  y_tensor = y_tensor.to(DEVICE)

  return X_tensor, y_tensor

def train(model, train_loader, optimizer, writer=None, num_epochs=10, weights=None):
  criterion = nn.CrossEntropyLoss(weight=weights)
  total_step = len(train_loader)
  loss_list = []
  acc_list = []
  early_stop = False
  patience = 5

  # criterion = SupConLoss()
  for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
      # Run the forward pass
      # images = torch.cat([images[0], images[1]], dim=0)
      outputs = model(images)
      print(outputs.shape)

      loss = criterion(outputs, labels)
      loss_list.append(loss.item())

      # Backprop and perform Adam optimisation
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      # Track the accuracy
      total = labels.size(0)
      _, predicted = torch.max(outputs.data, 1)
      right = (predicted == labels).sum().item()
      acc_list.append(right / total)
    
      if (i + 1) % 1 == 0:
        # if writer is None:
        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'
            .format(epoch + 1, num_epochs, i + 1, total_step, loss_list[i],
                  (right / total) * 100))
        
      if loss_list[i] - loss_list[i-1] <= 0.0001:
        early_stop = True
        count = 0
      if early_stop == True:
        if loss_list[i] - loss_list[i-1] > 0.0001 and count < patience:
          early_stop = False
        elif count >= patience:
          break
        count += 1

      del images

correct = []
incorrect = []


def get_cross_val_results(y_test, y_score, plot=False):
  fpr, tpr, thresholds = metrics.roc_curve(
        y_test, y_score, pos_label=1)

  auc_ = auc(fpr, tpr)
  pr, rc, thresholds = precision_recall_curve(y_test, y_score)
  pr_rc_auc = auc(rc, pr)
  if plot:
      return auc_, pr_rc_auc, pr, rc, fpr, tpr
  return auc_, pr_rc_auc, pr, rc

import torch.nn.functional as F
def test(model, loader, writer=None):
  # Test the model
  model.eval()
  confusion_matrix = np.zeros((2, 2))
  with torch.no_grad():
    right = 0
    total = 0
    idx = 0
    for images, labels in loader:
      
      outputs = model(images)
      probabilities = F.softmax(outputs, dim=1)[:, 1]
      y_score = probabilities.detach().numpy()
      print("LABELS ", labels)
      print('SCORE ', y_score)
      auc_, pr_rc_auc, pr, rc = get_cross_val_results(labels, y_score)

  return auc_, pr_rc_auc, labels, y_score

def get_train_test_acc():
  results = []
  profiles = ['7']
  for profile in profiles:
    weights, x_train, x_test, y_train, y_test = read_images_and_labels(profile)
    train_dataset = TensorDataset(x_train, y_train)
    test_dataset = TensorDataset(x_test, y_test)

    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,
                                shuffle=False)
    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, 
                            shuffle=False)
    model = None
    model = SticsModel()
    if torch.cuda.is_available():
      model.cuda()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    train(weights=weights)
    acc1, train_cm = test(model, train_loader)
    acc2, test_cm = test(model, test_loader)
    results.append([profile, acc1, train_cm, acc2, test_cm])

  for result in results:
    for i in results:
      print(i)
    print('\n')

from sklearn.model_selection import StratifiedKFold
def cross_validation(X, y, dataset, k_fold=5):
  train_score = pd.Series()
  val_score = []
  
  total_size = len(dataset)
  fraction = 1/k_fold
  seg = int(total_size * fraction)
  y_scores = []
  all_labels = []
  
  
  # tr:train,val:valid; r:right,l:left;  eg: trrr: right index of right side train subset 
  # index: [trll,trlr],[vall,valr],[trrl,trrr]
  cv_outer = StratifiedKFold(n_splits=k_fold,
        random_state=1, shuffle=True)
  for fold, (train_indices, val_indices) in enumerate(cv_outer.split(X, y)):
    print(train_indices, val_indices)
    model = None
    model = SticsModel()
    if torch.cuda.is_available():
      model.cuda()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
      
    train_set = torch.utils.data.dataset.Subset(dataset, train_indices)
    val_set = torch.utils.data.dataset.Subset(dataset, val_indices)

    train_loader = torch.utils.data.DataLoader(train_set, batch_size=16,
                                      shuffle=True, num_workers=4)
    val_loader = torch.utils.data.DataLoader(val_set, batch_size=len(val_indices),
                                      shuffle=True, num_workers=4)
    train_acc = train(model, train_loader, optimizer, num_epochs=10)
    train_score.at[fold] = train_acc
    val_auc, val_auc_pr, label, y_score = test(model, val_loader)
    all_labels = np.append(all_labels, label)
    y_scores = np.append(y_scores, y_score)
    val_score.append([val_auc, val_auc_pr])
  
  return train_score, val_score, all_labels, y_scores

def generalized_model():
  # profile = '17'
  all_profiles = ['7', '8', '9', '16', '17', '24', '28', '29', '32']
  # profile = ['28']
  X, y = read_images_and_labels_cv(all_profiles, combined=True)
  dataset = TensorDataset(X, y)
  train_score, val_score, all_labels, y_scores = cross_validation(X, y, dataset)
  print(val_score)
  print("AUC ROC, AUC_PR: ", val_score)
  print("Mean AUC-ROC: ", np.mean(val_score, axis=0)[0])
  print("Std AUC-ROC: ", np.std(val_score, axis=0)[0])

  print("Mean AUC-PR: ", np.mean(val_score, axis=0)[1])
  print("Std AUC-PR: ", np.std(val_score, axis=0)[1])

  # auc_, pr_rc_auc, pr, rc, fpr, tpr = get_cross_val_results(all_labels, y_scores, plot=True)
  # auc_score = roc_auc_score(all_labels, y_scores)
  # plot_outer_scores(profile, auc_score, pr_rc_auc, pr, rc, fpr, tpr)
  lw=2
  fpr, tpr, thresholds = metrics.roc_curve(all_labels, y_scores, pos_label=1)
  auc_ = metrics.auc(fpr, tpr)
  f1, ax1 = plt.subplots(1)
  ax1.plot(fpr, tpr, lw=lw, label="Area = %0.2f" % (auc_))
  ax1.set_xlim([0.0, 1.0])
  ax1.set_ylim([0.0, 1.05])
  
  ax1.set_xlabel('False Positive Rate')
  ax1.set_ylabel('True Positive Rate')
  ax1.legend(loc="lower right")
  f1.savefig('/cluster/home/t62928uhn/%s_CNN_ROC_AUC.png' % ('generalized'), dpi=300)

def personalized_model():
  all_profiles = ['7', '8', '9', '16', '17', '24', '28', '29', '32']
  results = []
  for profile in all_profiles:
    X, y = read_images_and_labels_cv(profile)
    dataset = TensorDataset(X, y)
    train_score, val_score, all_labels, y_scores = cross_validation(X, y, dataset)
    print(val_score)
    print("AUC ROC, AUC_PR: ", val_score)
    print("Mean AUC-ROC: ", np.mean(val_score, axis=0)[0])
    print("Std AUC-ROC: ", np.std(val_score, axis=0)[0])
    print("Mean AUC-PR: ", np.mean(val_score, axis=0)[1])
    print("Std AUC-PR: ", np.std(val_score, axis=0)[1])

    for score in val_score:
      results.append([profile, score[0], score[1]])
    results.append(['Means', np.mean(val_score, axis=0)[0], np.std(val_score, axis=0)[0]])
    results.append(['Stds', np.std(val_score, axis=0)[0], np.std(val_score, axis=0)[1]])

    lw=2
    fpr, tpr, thresholds = metrics.roc_curve(all_labels, y_scores, pos_label=1)
    auc_ = metrics.auc(fpr, tpr)
    pr, rc, thresholds = precision_recall_curve(all_labels, y_scores)
    pr_rc_auc = auc(rc, pr)

    results.append(["Pooled AUCs", auc_, pr_rc_auc])

    f1, ax1 = plt.subplots(1)
    ax1.plot(fpr, tpr, lw=lw, label="Area = %0.2f" % (auc_))
    ax1.set_xlim([0.0, 1.0])
    ax1.set_ylim([0.0, 1.05])
    
    ax1.set_xlabel('False Positive Rate')
    ax1.set_ylabel('True Positive Rate')
    ax1.legend(loc="lower right")
    f1.savefig('/cluster/home/t62928uhn/%s_CNN_ROC_AUC.png' % profile, dpi=300)

    results = pd.DataFrame(results)
    results.columns = ['profile', 'AUC-ROC', 'AUC-PR']
    results.to_csv('/cluster/home/t62928uhn/CNN_RESULTS_PERSONALIZED.csv')

def plot_outer_scores(profile, auc_, pr_rc_auc, pr, rc, fpr, tpr):
  f1, ax1 = plt.subplots(1)
  f2, ax2 = plt.subplots(1)
  lw = 2
  ax1.plot(fpr, tpr, lw=lw, label="Area = %0.2f" % (auc_))
  ax1.set_xlim([0.0, 1.0])
  ax1.set_ylim([0.0, 1.05])
  
  ax1.set_xlabel('False Positive Rate')
  ax1.set_ylabel('True Positive Rate')
  ax1.legend(loc="lower right")
  f1.savefig('/cluster/home/t62928uhn/%s_CNN_ROC_AUC.png' %  (profile), dpi=300)

  ax2.plot([0, 1], [baseline, baseline], color="navy", lw=lw, linestyle="--")
  ax2.plot(rc, pr, lw=lw, label="Area = %0.2f" % (pr_rc_auc))
  ax2.set_xlim([0.0, 1.0])
  ax2.set_ylim([0.0, 1.05])
  ax2.set_xlabel('Recall')
  ax2.set_ylabel('Precision')
  ax2.legend(loc="lower right")
  f2.savefig('/cluster/home/t62928uhn/clinical_data_included/%s_CNN_PR_AUC.png' %  (profile), dpi=300)  

def main(generalized=False, personalized=False):
  for profile in relevant_profiles:
      if generalized: generalized_model()
      if personalized: personalized_model()

if __name__ == "__main__":
    parser = setArgParser()
    arguments = parser.parse_args()
    main(generalized=arguments.generalized, personalized=arguments.personalized)
